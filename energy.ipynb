{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment of Cosmic Ray Energy Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import get_cuts, get_event_parameters, get_training_assessment_cut, r_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Assessment Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The keys will be the names of the models you wish to analyze.\n",
    "# The values will be the nuclei to assess for each model.\n",
    "MODEL_NAMES_AND_NUCLEI = {\n",
    "    'energy_baseline': 'phof'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IceTop-CNN folder in /data/user\n",
    "ICETOP_CNN_DATA_DIR = os.getenv('ICETOP_CNN_DATA_DIR')\n",
    "# Folder containing the models\n",
    "MODELS_FOLDER_PATH = os.path.join(ICETOP_CNN_DATA_DIR, 'models')\n",
    "# Folder containing the reconstructions\n",
    "RECONSTRUCTIONS_FOLDER_PATH = os.path.join(ICETOP_CNN_DATA_DIR, 'reconstructions', 'energy')\n",
    "# Folder containing the simulation data.\n",
    "SIMDATA_FOLDER_PATH = os.path.join(os.sep, 'data', 'user', 'fmcnally', 'icetop-cnn', 'simdata')\n",
    "\n",
    "# Various potential error messages.\n",
    "ERROR_MODELS_FOLDER_PATH_NOT_FOUND = 'Could not find models folder. Path specified: '\n",
    "ERROR_NO_MODELS_SELECTED = 'No models selected for analysis!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the models folder has been found.\n",
    "assert os.path.exists(MODELS_FOLDER_PATH), f'{ERROR_MODELS_FOLDER_PATH_NOT_FOUND}{MODELS_FOLDER_PATH}'\n",
    "\n",
    "# Get all model reconstructions .npy files.\n",
    "model_list = glob(os.path.join(RECONSTRUCTIONS_FOLDER_PATH, '*.npy'))\n",
    "# Trim parent directories and file extension for each.\n",
    "model_list = [os.path.splitext(os.path.basename(model))[0] for model in model_list]\n",
    "\n",
    "# Get all model parameters .json files.\n",
    "param_list = glob(os.path.join(MODELS_FOLDER_PATH, '*', '*.json'))\n",
    "# Trim parent directories and file extension for each.\n",
    "param_list = [os.path.splitext(os.path.basename(param))[0] for param in param_list]\n",
    "\n",
    "# Print models that have both .npy and .json files.\n",
    "print(f'Available models: {sorted(set(model_list).intersection(param_list))}')\n",
    "# Print models that have a .npy file but no .json file.\n",
    "print(f'Models without parameter files: {sorted(set(model_list).difference(param_list))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store model parameters.\n",
    "model_parameters = {}\n",
    "# Loop over model names and their corresponding assessment nuclei.\n",
    "for model_name, assessment_nuclei in MODEL_NAMES_AND_NUCLEI.items():\n",
    "    \n",
    "    # Construct the full .json model path.\n",
    "    model_path = os.path.join(MODELS_FOLDER_PATH, model_name, model_name + '.json')\n",
    "\n",
    "    # Ensure that the model is found (no typos).\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f'WARNING: Model {model_name} not found at {MODELS_FOLDER_PATH}')\n",
    "        continue\n",
    "\n",
    "    # Load model parameters and save into the dictionary.\n",
    "    with open(model_path, 'r') as f:\n",
    "        model_parameters[model_name] = json.load(f)\n",
    "    # Add assessment nuclei to model parameters. Sort the composition string into a predictable order.\n",
    "    model_parameters[model_name].update(\n",
    "        {'assessment_nuclei': ''.join(sorted(assessment_nuclei, key=lambda c: list('phof').index(c)))})\n",
    "    \n",
    "    # Print an entry for each model.\n",
    "    print(f'{model_name:>{max(map(len, MODEL_NAMES_AND_NUCLEI))}} : {model_parameters[model_name]}')\n",
    "\n",
    "# Ensure that at least one valid model has been selected for assessment.\n",
    "assert len(model_parameters), ERROR_NO_MODELS_SELECTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Event Parameters\n",
    "\n",
    "This might takes a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must always load all nuclei. Each model will have a unique preference for which events to assess on.\n",
    "# Undesired events are cut next.\n",
    "event_parameters = get_event_parameters(SIMDATA_FOLDER_PATH, composition='phof')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Reconstructions and Generate Cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the nuclei string representations to their atomic weights.\n",
    "comp_conversion = {'p':1, 'h':4, 'o':16, 'f':56}\n",
    "\n",
    "# Build dictionaries for model reconstructions and events cuts.\n",
    "reconstructions, cuts = {}, {}\n",
    "# Loop over model names and their corresponding parameters.\n",
    "for model_name, model_prep in model_parameters.items():\n",
    "\n",
    "    # Load model predictions. It is important to flatten the predictions into a one-dimensional array.\n",
    "    reconstruction = np.load(os.path.join(RECONSTRUCTIONS_FOLDER_PATH, model_name + '.npy')).flatten()\n",
    "\n",
    "    # Get the model-specific assessment cut.\n",
    "    model_cut = get_training_assessment_cut(event_parameters, 'assessment', model_prep)\n",
    "\n",
    "    # Get the list of atomic weights for the desired nuclei.\n",
    "    nuclei_to_assess = [comp_conversion[nuclei] for nuclei in model_prep['assessment_nuclei']]\n",
    "\n",
    "    # Apply the nuclei cut to the model predictions.\n",
    "    reconstruction = reconstruction[np.isin(event_parameters['comp'][model_cut], nuclei_to_assess)]\n",
    "    # Save the model predictions.\n",
    "    reconstructions[model_name] = reconstruction\n",
    "\n",
    "    # Get the model-specific events cut.\n",
    "    model_cut = np.isin(event_parameters['comp'], nuclei_to_assess) * model_cut\n",
    "    # Save the model-specific events cut.\n",
    "    cuts[model_name] = model_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG FOR STATS\n",
    "log_difference, reco_percent = .1, 15\n",
    "cut_names = ['Uncut', 'Quality']\n",
    "\n",
    "# Loop over each model's name and their corresponding reconstructed energies.\n",
    "for model_name, reconstructed_energy in reconstructions.items():\n",
    "    # Print a header entry for each model.\n",
    "    print(f'Stats for {model_name}:')\n",
    "\n",
    "    # Loop over the cuts.\n",
    "    for cut_name in cut_names:\n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "\n",
    "        # Calculate the event-wise difference in reconstructed vs true logged energies.\n",
    "        diff = reconstructed_energy[reconstructions_cut] - event_parameters['energy'][events_cut]\n",
    "        # Calculate the event-wise difference in reconstructed vs true unlogged energies.\n",
    "        true_diff = r_log(reconstructed_energy[reconstructions_cut]) - r_log(event_parameters['energy'][events_cut])\n",
    "        # Calculate the event-wise percent difference in reconstructed vs true unlogged energies.\n",
    "        percent_diff = 100 * true_diff / r_log(event_parameters['energy'][events_cut])\n",
    "\n",
    "        # Calculate energy resolution statistics for the logged energy difference.\n",
    "        std_dev_lo, median, std_dev_hi = np.percentile(diff, (16,50,84))\n",
    "\n",
    "        # Calculate the percentage of events whose difference in true and reconstructed energies is less than \"log_difference\".\n",
    "        diff_percentage = 100 * len(list(filter(lambda x: abs(x) <= log_difference, diff))) / len(diff)\n",
    "        # Calculate the percentage of events whose reconstructed energies are within \"reco_percent\"% of their true energies.\n",
    "        percent_diff_percentage = 100 * len(list(filter(lambda x: abs(x) <= reco_percent, percent_diff))) / len(percent_diff)\n",
    "\n",
    "        # Print the summary statistics.\n",
    "        print(f'{\" \"*2}{cut_name}')\n",
    "        print(f'{\" \"*4}Energy resolution: {median:.3f} +{std_dev_hi:.3f} {std_dev_lo:.3f}')\n",
    "        print(f'{\" \"*4}Events reconstructed within a {log_difference} difference in order of magnitude: {diff_percentage:.2f}%')\n",
    "        print(f'{\" \"*4}Events reconstructed within {reco_percent}% of their true energies: {percent_diff_percentage:.2f}%')\n",
    "    \n",
    "    print() # Newline for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logged Energy Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG FOR PLOT\n",
    "cut_names = ['Uncut', 'Quality']\n",
    "hist_args = {'range':(-2,2), 'bins':121, 'density':True, 'histtype':'step', 'log':True, 'linewidth':4}\n",
    "\n",
    "# CONFIG FOR TEXT\n",
    "title_params = {'fontsize':28}\n",
    "label_params = {'fontsize':20}\n",
    "legend_params = {'fontsize':14}\n",
    "\n",
    "# VARIABLES FOR PLOT\n",
    "ncols = len(cut_names)\n",
    "\n",
    "# Create the plot/subplots.\n",
    "fig, axs = plt.subplots(figsize=(13*ncols, 8), ncols=ncols)\n",
    "\n",
    "# If we are assessing only one cut, add an artificial dimension.\n",
    "if ncols == 1: axs = np.array([axs])\n",
    "# Loop over the cuts.\n",
    "for ax, cut_name in zip(axs, cut_names):\n",
    "    # Loop over the models.\n",
    "    for model_name in model_parameters:\n",
    "        \n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "        \n",
    "        # Plot the logged difference histogram.\n",
    "        ax.hist((reconstructions[model_name][reconstructions_cut] - event_parameters['energy'][events_cut]),\n",
    "                label=model_name, **hist_args)\n",
    "\n",
    "    # Plot a vertical line through the center of each plot.\n",
    "    # This line designates the ideal reconstruction.\n",
    "    ax.axvline()\n",
    "\n",
    "    # Decorate the plot.\n",
    "    ax.set_title(f'Energy Resolution ({cut_name})', **title_params)\n",
    "    ax.set_xlabel(r'$\\log_{10}(E_{\\mathrm{reco}}/\\mathrm{GeV}) - \\log_{10}(E_{\\mathrm{true}}/\\mathrm{GeV})$', **label_params)\n",
    "    ax.set_ylabel('Density', **label_params)\n",
    "    ax.legend(**legend_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zoomed, Unlogged Energy Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG FOR PLOT\n",
    "cut_names = ['Uncut', 'Quality']\n",
    "hist_args = {'range':(-1,1), 'bins':121, 'density':True, 'histtype':'step', 'log':False, 'linewidth':4}\n",
    "\n",
    "# CONFIG FOR TEXT\n",
    "title_params = {'fontsize':28}\n",
    "label_params = {'fontsize':20}\n",
    "legend_params = {'fontsize':14}\n",
    "\n",
    "# VARIABLES FOR PLOT\n",
    "ncols = len(cut_names)\n",
    "\n",
    "# Create the plot/subplots.\n",
    "fig, axs = plt.subplots(figsize=(13*ncols, 8), ncols=ncols)\n",
    "\n",
    "# If we are assessing only one cut, add an artificial dimension.\n",
    "if ncols == 1: axs = np.array([axs])\n",
    "# Loop over the cuts.\n",
    "for ax, cut_name in zip(axs, cut_names):\n",
    "    # Loop over the models.\n",
    "    for model_name in model_parameters:\n",
    "\n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "\n",
    "        # Plot the zoomed and unlogged difference histogram.\n",
    "        ax.hist((reconstructions[model_name][reconstructions_cut] - event_parameters['energy'][events_cut]),\n",
    "                label=model_name, **hist_args)\n",
    "\n",
    "    # Plot a vertical line through the center of each plot.\n",
    "    # This line designates the ideal reconstruction.\n",
    "    ax.axvline()\n",
    "    \n",
    "    # Decorate the plot.\n",
    "    ax.set_title(f'Energy Resolution ({cut_name})', **title_params)\n",
    "    ax.set_xlabel(r'$\\log_{10}(E_{\\mathrm{reco}}/\\mathrm{GeV}) - \\log_{10}(E_{\\mathrm{true}}/\\mathrm{GeV})$', **label_params)\n",
    "    ax.set_ylabel('Density', **label_params)\n",
    "    ax.legend(**legend_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-Dimensional Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG FOR PLOT\n",
    "cut_names = ['Uncut', 'Quality']\n",
    "energy_range = (5, 8)\n",
    "nbins = 180\n",
    "\n",
    "# CONFIG FOR TEXT\n",
    "title_params = {'fontsize':28}\n",
    "label_params = {'fontsize':20}\n",
    "tick_params = {'axis':'both', 'direction':'out', 'labelsize':14}\n",
    "\n",
    "# VARIABLES FOR PLOT\n",
    "ncols, nrows = len(cut_names), len(model_parameters)\n",
    "bin_edges = np.linspace(*energy_range, nbins+1)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# Create the plot/subplots.\n",
    "fig, axs = plt.subplots(figsize=(13*ncols, 10*nrows), ncols=ncols, nrows=nrows, sharex=True, sharey=True)\n",
    "\n",
    "# If we are assessing only one model, add an artificial dimension.\n",
    "if nrows == 1: axs = np.array([axs])\n",
    "# Loop over the models.\n",
    "for model_ax, model_name in zip(axs, model_parameters):\n",
    "    # If we are assessing only one cut, add an artificial dimension.\n",
    "    if ncols == 1: model_ax = np.array([model_ax])\n",
    "    # Loop over the cuts.\n",
    "    for ax, cut_name in zip(model_ax, cut_names):\n",
    "        \n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "\n",
    "        # Compute the 2D histogram.\n",
    "        hist, _, _ = np.histogram2d(reconstructions[model_name][reconstructions_cut], event_parameters['energy'][events_cut],\n",
    "                                    bins=(bin_edges, bin_edges))\n",
    "\n",
    "        # Normalize the histogram along each true energy bin.\n",
    "        # This special way of dividing avoids any divide-by-zero errors.\n",
    "        hist = np.divide(hist, np.sum(hist, axis=0), out=np.zeros_like(hist), where=np.sum(hist, axis=0) != 0)\n",
    "        \n",
    "        # Plot the logged 2D histogram.\n",
    "        # Ignore divide-by-zero errors caused by a value of 0 in any bin.\n",
    "        with np.errstate(divide='ignore'):\n",
    "            im = ax.imshow(np.log10(hist), extent=(*energy_range, *energy_range), vmin=-3.5, vmax=-0.5, origin='lower')\n",
    "\n",
    "        # Plot a diagonal line through each plot.\n",
    "        # This line designates the ideal reconstruction.\n",
    "        ax.plot(energy_range, energy_range, color='black', linestyle=':')\n",
    "\n",
    "        # Create contour lines for one and two standard deviations on either side of the median.\n",
    "        # These values are determined for each true energy bin.\n",
    "        contour_values = [.025, .16, .84, .975]\n",
    "        # Take the cumulative sum of the array, a range [0-1]. We can then look at where we would insert\n",
    "        #   each contour value to determine the heights for the stairs. For columns with no entries, the\n",
    "        #   resulting list indices will be past the end of the list. We can use the indices modulo the number\n",
    "        #   of bins to set the problematic list indices to 0.\n",
    "        contour_indices = np.asarray([np.searchsorted(col, contour_values) for col in np.cumsum(hist, axis=0).transpose()]) % nbins\n",
    "        for contour in contour_indices.transpose():\n",
    "            ax.stairs(bin_centers[contour], edges=bin_edges, color='red', linestyle='--')\n",
    "        \n",
    "        # Create a colorbar.\n",
    "        cbar = fig.colorbar(im, ax=ax)\n",
    "        cbar.ax.tick_params(**tick_params)\n",
    "        \n",
    "        # Set the y-axis limits to be the energy range.\n",
    "        ax.set_ylim(energy_range)\n",
    "\n",
    "        # Decorate the plot.\n",
    "        ax.set_title(f'{model_name} ({cut_name})', **title_params)\n",
    "        ax.set_xlabel(r'$\\log_{10}(E_{\\mathrm{true}}/\\mathrm{GeV})$', **label_params)\n",
    "        ax.set_ylabel(r'$\\log_{10}(E_{\\mathrm{reco}}/\\mathrm{GeV})$', **label_params)\n",
    "        ax.tick_params(**tick_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy Resolution as a Function of Zenith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG FOR PLOT\n",
    "cut_names = ['Uncut']\n",
    "cosz_range = (0.4, 1)\n",
    "nbins = 19\n",
    "comparison_reconstruction = 'laputop'\n",
    "errorbar_args = {'fmt':'.', 'markersize':40, 'elinewidth':2, 'capsize':10, 'capthick':2}\n",
    "\n",
    "# CONFIG FOR TEXT\n",
    "title_params = {'fontsize':28}\n",
    "label_params = {'fontsize':20}\n",
    "tick_params = {'labelsize':14}\n",
    "legend_params = {'fontsize':14, 'markerscale':0.5}\n",
    "\n",
    "# VARIABLES FOR PLOT\n",
    "ncols = len(cut_names)\n",
    "bin_edges = np.linspace(*cosz_range, nbins+1)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# Create the plot/subplots.\n",
    "fig, axs = plt.subplots(figsize=(26, 10), ncols=ncols)\n",
    "\n",
    "# If we are assessing only one cut, add an artificial dimension.\n",
    "if ncols == 1: axs = np.array([axs])\n",
    "# Loop over the cuts.\n",
    "for ax, cut_name in zip(axs, cut_names):\n",
    "    # Loop over the models.\n",
    "    for model_name in model_parameters:\n",
    "        \n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "    \n",
    "        # Get the true zenith angle for each event.\n",
    "        zenith = np.pi - event_parameters[f'{comparison_reconstruction}_dir'][events_cut][..., 0].astype(np.float32)\n",
    "        # Bin each event by its true zenith.\n",
    "        binned_zenith = np.digitize(np.cos(zenith), bin_edges)\n",
    "\n",
    "        # Create an empty array to hold the statistics for each bin.\n",
    "        binned_statistics = np.zeros((nbins, 3))\n",
    "        # Loop over the bins.\n",
    "        for bin in range(nbins):\n",
    "            # Create a bin-specific cut. This is how we will generate statistics for each bin.\n",
    "            bin_cut = (binned_zenith == bin)\n",
    "            # No data for a particular bin means we can skip it.\n",
    "            if not np.any(bin_cut): continue\n",
    "            # Generate the statistics for each bin.\n",
    "            binned_statistics[bin] = np.percentile(\n",
    "                reconstructions[model_name][reconstructions_cut][bin_cut] - event_parameters['energy'][events_cut][bin_cut],\n",
    "                (50, 16, 84))\n",
    "\n",
    "        # Extract each statistic separately.\n",
    "        median, err_min, err_max = np.transpose(binned_statistics)\n",
    "        # Plot the energy resolution as a function of zenith.\n",
    "        ax.errorbar(bin_centers, median, yerr=(median-err_min, err_max-median), label=model_name, **errorbar_args)\n",
    "    \n",
    "    # Plot a horizontal line through the center of each plot.\n",
    "    # This line designates the ideal reconstruction.\n",
    "    ax.axhline(color='black', ls='--')\n",
    "    \n",
    "    # Set the y-axis limits to be within a reasonable range.\n",
    "    ax.set_ylim(-0.25, 0.25)\n",
    "    \n",
    "    # Decorate the plot.\n",
    "    ax.set_title(f'Energy Resolution v. Zenith ({cut_name})', **title_params)\n",
    "    ax.set_xlabel(r'$\\cos(\\theta)$', loc='right', **label_params)\n",
    "    ax.set_ylabel(r'$\\log_{10}(E_{\\mathrm{reco}}/\\mathrm{GeV}) - \\log_{10}(E_{\\mathrm{true}}/\\mathrm{GeV})$', **label_params)\n",
    "    ax.tick_params(**tick_params)\n",
    "    ax.legend(**legend_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IceTop-CNN",
   "language": "python",
   "name": "icetop-cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
