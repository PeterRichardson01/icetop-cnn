{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosmic Ray Energy Reconstruction Using Convolutional Neural Network\n",
    "## Utilizing Charge, Time, and Zenith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer\n",
    "from data_tools import data_prep, get_reco_nan_filter, load_preprocessed\n",
    "from keras.callbacks import CSVLogger, EarlyStopping\n",
    "from keras.layers import BatchNormalization, Concatenate, Conv2D, Dense, Flatten, Input, MaxPooling2D\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Baseline data prep ###\n",
    "\n",
    "# Edit these parameters\n",
    "prep = {'clc':True, 'sta5':False, 'q':None, 't':None, 't_shift':True, 't_clip':0, 'normed':True, 'reco':None, 'cosz':False}\n",
    "\n",
    "# Set the number of models to train under this CNN\n",
    "num_models_to_train = 1\n",
    "\n",
    "# Name for model(s)\n",
    "model_name = ''\n",
    "\n",
    "# Set the number of epochs the model(s) should run for\n",
    "# Actual result may differ due to early stopping\n",
    "num_epochs = 100\n",
    "\n",
    "# Loss metric to use for training\n",
    "# Suggestion to experiment with 'huber_loss', especially for MASTER'S THESIS model\n",
    "loss_function = 'mean_squared_error'\n",
    "\n",
    "# Optimizer to user for training\n",
    "optimizer = 'adam'\n",
    "\n",
    "# Other loss metrics to analyze while training\n",
    "# Only for user to monitor - have no effect on model training\n",
    "metrics = ['mae','mse']\n",
    "\n",
    "# File directory to folder that holds models\n",
    "model_prefix = os.getcwd()+'/models'\n",
    "\n",
    "# File directory to folder that holds simulation data \n",
    "sim_prefix = os.getcwd()+'/simdata'\n",
    "\n",
    "# Booleans for easier to read conditionals - no need to change this\n",
    "has_reco, has_time = prep['reco'] != None, prep['t'] != False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load simulation data from files for training\n",
    "x, y = load_preprocessed(sim_prefix, 'train')\n",
    "\n",
    "# Prepare simulation data\n",
    "x_i, idx = data_prep(x, y, **prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter NaNs from reconstruction data ###\n",
    "if has_reco:\n",
    "    reco_nan_filter = get_reco_nan_filter(prep['reco'], y)\n",
    "\n",
    "    for i, _ in enumerate(x_i):\n",
    "        x_i[i] = x_i[i][reco_nan_filter]\n",
    "        \n",
    "    for key in y.keys():\n",
    "        y[key] = y[key][reco_nan_filter]\n",
    "    \n",
    "    nan_loss = (len(reco_nan_filter)-sum(reco_nan_filter)) / len(reco_nan_filter) * 100\n",
    "    print('Percentage of events with a NaN in reconstruction: %.02f' % nan_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \\/ MASTER'S THESIS \\/ ###\n",
    "\"\"\"\n",
    "# Charge is always included as a parameter in the model - highest correlation with energy of all inputs\n",
    "# Charge input layer\n",
    "charge_input = Input(shape=(10,10,idx), name='charge')\n",
    "# Starts off with two Convolutional layers, each one has double the neurons of the previous\n",
    "q_conv1_layer = BatchNormalization()(Conv2D(64, kernel_size=3, padding='same', activation='relu')(charge_input))\n",
    "q_conv2_layer = BatchNormalization()(Conv2D(128, kernel_size=3, padding='same', activation='relu')(q_conv1_layer))\n",
    "# A Maxpooling layer is applied after two Convolutional layers\n",
    "q_maxpooling1_layer = MaxPooling2D(pool_size=2, strides=1, padding='same')(q_conv2_layer)\n",
    "# Continues with two more Convolutional layers, each one has double the neurons of the previous\n",
    "q_conv3_layer = BatchNormalization()(Conv2D(256, kernel_size=3, padding='same', activation='relu')(q_maxpooling1_layer))\n",
    "q_conv4_layer = BatchNormalization()(Conv2D(512, kernel_size=3, padding='same', activation='relu')(q_conv3_layer))\n",
    "# A second Maxpooling layer is applied after two additional Convolutional layers\n",
    "q_maxpooling2_layer = MaxPooling2D(pool_size=2, strides=1, padding='same')(q_conv4_layer)\n",
    "# Continues with one more Convolutional layer with again double the neurons\n",
    "q_conv5_layer = BatchNormalization()(Conv2D(1024, kernel_size=3, padding='same', activation='relu')(q_maxpooling2_layer))\n",
    "# A third maxpooling layer is applied after the final Convolutional layer\n",
    "q_maxpooling3_layer = MaxPooling2D(pool_size=2, strides=1, padding='same')(q_conv5_layer)\n",
    "# Layer is Flattened before Concatenated\n",
    "q_flat_layer = Flatten()(q_maxpooling3_layer)\n",
    "\n",
    "# Time has been found to provide information comparable to Zenith when given to more advanced CNNs\n",
    "if has_time: # Whether time has been included as a parameter in the model\n",
    "    # Time input layer \n",
    "    time_input = Input(shape=(10,10,x_i[0].shape[-1]-idx), name='time')\n",
    "    # Starts off with two Convolutional layers, each one has double the neurons of the previous\n",
    "    t_conv1_layer = BatchNormalization()(Conv2D(64, kernel_size=3, padding='same', activation='relu')(time_input))\n",
    "    t_conv2_layer = BatchNormalization()(Conv2D(128, kernel_size=3, padding='same', activation='relu')(t_conv1_layer))\n",
    "    # A Maxpooling layer is applied after two Convolutional layers\n",
    "    t_maxpooling1_layer = MaxPooling2D(pool_size=2, strides=1, padding='same')(t_conv2_layer)\n",
    "    # Continues with two more Convolutional layers, each one has double the neurons of the previous\n",
    "    t_conv3_layer = BatchNormalization()(Conv2D(256, kernel_size=3, padding='same', activation='relu')(t_maxpooling1_layer))\n",
    "    t_conv4_layer = BatchNormalization()(Conv2D(512, kernel_size=3, padding='same', activation='relu')(t_conv3_layer))\n",
    "    # A second Maxpooling layer is applied after two additional Convolutional layers\n",
    "    t_maxpooling2_layer = MaxPooling2D(pool_size=2, strides=1, padding='same')(t_conv4_layer)\n",
    "    # Continues with one more Convolutional layer with again double the neurons\n",
    "    t_conv5_layer = BatchNormalization()(Conv2D(1024, kernel_size=3, padding='same', activation='relu')(t_maxpooling2_layer))\n",
    "    # A third maxpooling layer is applied after the final Convolutional layer\n",
    "    t_maxpooling3_layer = MaxPooling2D(pool_size=2, strides=1, padding='same')(t_conv5_layer)\n",
    "    # Layer is Flattened before Concatenated\n",
    "    t_flat_layer = Flatten()(t_maxpooling3_layer)\n",
    "\"\"\"\n",
    "### \\/ BASELINE \\/ ###\n",
    "# Charge is always included as a parameter in the model - highest correlation with energy of all inputs\n",
    "# Charge input layer\n",
    "charge_input = Input(shape=(10,10,idx), name='charge')\n",
    "# Starts off with three Convolutional layers, each one has half the neurons of the previous\n",
    "q_conv1_layer = Conv2D(64, kernel_size=3, padding='same', activation='relu')(charge_input)\n",
    "q_conv2_layer = Conv2D(32, kernel_size=3, padding='same', activation='relu')(q_conv1_layer)\n",
    "q_conv3_layer = Conv2D(16, kernel_size=3, padding='same', activation='relu')(q_conv2_layer)\n",
    "# Layer is Flattened before Concatenated\n",
    "q_flat_layer = Flatten()(q_conv3_layer)\n",
    "\n",
    "# Time has been found to provide information comparable to Zenith when given to more advanced CNNs\n",
    "if has_time: # Whether time has been included as a parameter in the model\n",
    "    # Time input layer \n",
    "    time_input = Input(shape=(10,10,x_i[0].shape[-1]-idx), name='time')\n",
    "    # Starts off with three Convolutional layers, each one has half the neurons of the previous\n",
    "    t_conv1_layer = Conv2D(64, kernel_size=3, padding='same', activation='relu')(time_input)\n",
    "    t_conv2_layer = Conv2D(32, kernel_size=3, padding='same', activation='relu')(t_conv1_layer)\n",
    "    t_conv3_layer = Conv2D(16, kernel_size=3, padding='same', activation='relu')(t_conv2_layer)\n",
    "    # Layer is Flattened before Concatenated\n",
    "    t_flat_layer = Flatten()(t_conv3_layer)\n",
    "\n",
    "\n",
    "\n",
    "### \\/ BOTH \\/ ###\n",
    "# Including Zenith, although cheatsy, is (as of now), the best way to improve the model\n",
    "if has_reco:\n",
    "    # Zenith input layer\n",
    "    zenith_input = Input(shape=(1), name='zenith')\n",
    "\n",
    "# Flat layers are Concatenated before being passed into Dense layers\n",
    "if has_time:\n",
    "    if has_reco:\n",
    "        concat_layer = Concatenate()([q_flat_layer, t_flat_layer, zenith_input])\n",
    "    else:\n",
    "        concat_layer = Concatenate()([q_flat_layer, t_flat_layer])\n",
    "elif not has_reco:\n",
    "        raise Exception('Why train the model on charge alone? It is not worth it, promise.')\n",
    "else:\n",
    "    concat_layer = Concatenate()([q_flat_layer, zenith_input])\n",
    "\n",
    "\n",
    "### \\/ MASTER'S THESIS \\/ ###\n",
    "\"\"\"\n",
    "# The Concatenated layers run through one Dense layer\n",
    "dense_layer = BatchNormalization()(Dense(2048, activation='relu')(concat_layer))\n",
    "# This last Dense layer is the output of the model\n",
    "output = Dense(1)(dense_layer)\n",
    "\"\"\"\n",
    "### \\/ BASELINE \\/ ###\n",
    "# The Concatenated layers run through three Dense layers\n",
    "dense1_layer = Dense(256, activation='relu')(concat_layer)\n",
    "dense2_layer = Dense(256, activation='relu')(dense1_layer)\n",
    "dense3_layer = Dense(256, activation='relu')(dense2_layer)\n",
    "# This last Dense layer is the output of the model\n",
    "output = Dense(1)(dense3_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing, Training, and Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_model in range(num_models_to_train):\n",
    "\n",
    "    ### Prepare model for compilation ###\n",
    "    # Copy model name\n",
    "    name = model_name\n",
    "\n",
    "    # Make sure we don't overwrite any models\n",
    "    i = 0\n",
    "    while(os.path.exists('%s/%s.h5' % (model_prefix, name+str(i)))): i += 1\n",
    "    name += str(i)\n",
    "    \n",
    "    if has_time:\n",
    "        if has_reco:\n",
    "            model = Model(inputs=[charge_input, time_input, zenith_input], outputs=output, name=name)\n",
    "            fit_inputs = {'charge':x_i[0][...,:idx], 'time':x_i[0][...,idx:], 'zenith':x_i[1].reshape(-1,1)}\n",
    "        else:\n",
    "            model = Model(inputs=[charge_input, time_input], outputs=output, name=name)\n",
    "            fit_inputs = {'charge':x_i[...,:idx], 'time':x_i[...,idx:]}\n",
    "    else:\n",
    "        model = Model(inputs=[charge_input, zenith_input], outputs=output, name=name)\n",
    "        fit_inputs = {'charge':x_i[0][...,:idx], 'zenith':x_i[1].reshape(-1,1)}\n",
    "\n",
    "    model.compile(loss=loss_function, optimizer=optimizer, metrics=metrics)\n",
    "    #model.summary()\n",
    "\n",
    "\n",
    "    # Training\n",
    "    # Earlystoping stops the model from training when it starts to overfit to the data\n",
    "    # The main parameter to change is patience - number of epochs where val_loss does not improve before stopping\n",
    "    # Should experiment with restore_best_weights\n",
    "    early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min', baseline=None, restore_best_weights=False) \n",
    "    csv_logger = CSVLogger('%s/%s' % (model_prefix, name))\n",
    "\n",
    "    history = model.fit(fit_inputs, y=y['energy'], epochs=num_epochs, validation_split=0.15, callbacks=[early_stop, csv_logger])\n",
    "\n",
    "    # Save the model results as a .npy and .h5 file\n",
    "    model.save('%s/%s.h5' % (model_prefix, name))\n",
    "    np.save('%s/%s.npy' % (model_prefix, name), prep)\n",
    "\n",
    "    # Open a .csv file and write the results of the best epoch\n",
    "    val_loss = np.min(history.history['val_loss'])\n",
    "    index = history.history['val_loss'].index(val_loss)\n",
    "    loss = history.history['loss'][index]\n",
    "    new_row = [name, index, loss, val_loss]\n",
    "    with open('models/results.csv', 'a') as f:\n",
    "        writer(f).writerow(new_row)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31925b19b5fe5a511cc521412368d42da4de764f685469458a6425fd9ad7937b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
