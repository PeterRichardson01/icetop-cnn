{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment of Cosmic Ray Composition Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import get_cuts, get_event_parameters, get_training_assessment_cut"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Assessment Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The keys will be the names of the models you wish to analyze.\n",
    "# The values will be the nuclei to assess for each model.\n",
    "MODEL_NAMES_AND_NUCLEI = {\n",
    "    'comp_baseline': 'phof'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IceTop-CNN folder in /data/user\n",
    "ICETOP_CNN_DATA_DIR = os.getenv('ICETOP_CNN_DATA_DIR')\n",
    "# Folder containing the models\n",
    "MODELS_FOLDER_PATH = os.path.join(ICETOP_CNN_DATA_DIR, 'models')\n",
    "# Folder containing the reconstructions\n",
    "RECONSTRUCTIONS_FOLDER_PATH = os.path.join(ICETOP_CNN_DATA_DIR, 'reconstructions', 'comp')\n",
    "# Folder containing the simulation data.\n",
    "SIMDATA_FOLDER_PATH = os.path.join(os.sep, 'data', 'user', 'fmcnally', 'icetop-cnn', 'simdata')\n",
    "\n",
    "# Various potential error messages.\n",
    "ERROR_MODELS_FOLDER_PATH_NOT_FOUND = 'Could not find models folder. Path specified: '\n",
    "ERROR_NO_MODELS_SELECTED = 'No models selected for analysis!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the models folder has been found.\n",
    "assert os.path.exists(MODELS_FOLDER_PATH), f'{ERROR_MODELS_FOLDER_PATH_NOT_FOUND}{MODELS_FOLDER_PATH}'\n",
    "\n",
    "# Get all model reconstructions .npy files.\n",
    "model_list = glob(os.path.join(RECONSTRUCTIONS_FOLDER_PATH, '*.npy'))\n",
    "# Trim parent directories and file extension for each.\n",
    "model_list = [os.path.splitext(os.path.basename(model))[0] for model in model_list]\n",
    "\n",
    "# Get all model parameters .json files.\n",
    "param_list = glob(os.path.join(MODELS_FOLDER_PATH, '*', '*.json'))\n",
    "# Trim parent directories and file extension for each.\n",
    "param_list = [os.path.splitext(os.path.basename(param))[0] for param in param_list]\n",
    "\n",
    "# Print models that have both .npy and .json files.\n",
    "print(f'Available models: {sorted(set(model_list).intersection(param_list))}')\n",
    "# Print models that have a .npy file but no .json file.\n",
    "print(f'Models without parameter files: {sorted(set(model_list).difference(param_list))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store model parameters.\n",
    "model_parameters = {}\n",
    "# Loop over model names and their corresponding assessment nuclei.\n",
    "for model_name, assessment_nuclei in MODEL_NAMES_AND_NUCLEI.items():\n",
    "    \n",
    "    # Construct the full .json model path.\n",
    "    model_path = os.path.join(MODELS_FOLDER_PATH, model_name, model_name + '.json')\n",
    "\n",
    "    # Ensure that the model is found (no typos).\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f'WARNING: Model {model_name} not found at {MODELS_FOLDER_PATH}')\n",
    "        continue\n",
    "\n",
    "    # Load model parameters and save into the dictionary.\n",
    "    with open(model_path, 'r') as f:\n",
    "        model_parameters[model_name] = json.load(f)\n",
    "    # Add assessment nuclei to model parameters. Sort the composition string into a predictable order.\n",
    "    model_parameters[model_name].update(\n",
    "        {'assessment_nuclei': ''.join(sorted(assessment_nuclei, key=lambda c: list('phof').index(c)))})\n",
    "    \n",
    "    # Print an entry for each model.\n",
    "    print(f'{model_name:>{max(map(len, MODEL_NAMES_AND_NUCLEI))}} : {model_parameters[model_name]}')\n",
    "\n",
    "# Ensure that at least one valid model has been selected for assessment.\n",
    "assert len(model_parameters), ERROR_NO_MODELS_SELECTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Event Parameters\n",
    "\n",
    "This might takes a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must always load all nuclei. Each model will have a unique preference for which events to assess on.\n",
    "# Undesired events are cut next.\n",
    "event_parameters = get_event_parameters(SIMDATA_FOLDER_PATH, composition='phof')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Reconstructions and Generate Cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the nuclei string representations to their atomic weights.\n",
    "comp_conversion = {'p':1, 'h':4, 'o':16, 'f':56}\n",
    "\n",
    "# Build dictionaries for model reconstructions and events cuts.\n",
    "reconstructions, cuts = {}, {}\n",
    "# Loop over model names and their corresponding parameters.\n",
    "for model_name, model_prep in model_parameters.items():\n",
    "\n",
    "    # Load model predictions. It is important to flatten the predictions into a one-dimensional array.\n",
    "    reconstruction = np.load(os.path.join(RECONSTRUCTIONS_FOLDER_PATH, model_name + '.npy'))\n",
    "\n",
    "    # Get the model-specific assessment cut.\n",
    "    model_cut = get_training_assessment_cut(event_parameters, 'assessment', model_prep)\n",
    "\n",
    "    # Get the list of atomic weights for the desired nuclei.\n",
    "    nuclei_to_assess = [comp_conversion[nuclei] for nuclei in model_prep['assessment_nuclei']]\n",
    "\n",
    "    # Apply the nuclei cut to the model predictions.\n",
    "    reconstruction = reconstruction[np.isin(event_parameters['comp'][model_cut], nuclei_to_assess)]\n",
    "    # Save the model predictions.\n",
    "    reconstructions[model_name] = np.argmax(reconstruction, axis=1)\n",
    "\n",
    "    # Get the model-specific events cut.\n",
    "    model_cut = np.isin(event_parameters['comp'], nuclei_to_assess) * model_cut\n",
    "    # Save the model-specific events cut.\n",
    "    cuts[model_name] = model_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG FOR PLOT\n",
    "cut_names = ['Uncut', 'Quality']\n",
    "\n",
    "# CONFIG FOR TEXT\n",
    "text_params = {'fontsize':20}\n",
    "title_params = {'fontsize':28, 'pad':14}\n",
    "label_params = {'fontsize':20}\n",
    "tick_params = {'labelsize':14, 'pad':8, 'top':True, 'labeltop':True, 'bottom':False, 'labelbottom':False}\n",
    "\n",
    "# VARIABLES FOR PLOT\n",
    "ncols, nrows = len(cut_names), len(model_parameters)\n",
    "weights_to_categories = {1:0, 4:1, 16:2, 56:3}\n",
    "categories_to_comp = {0:'p', 1:'h', 2:'o', 3:'f'}\n",
    "\n",
    "# Create the plot/subplots.\n",
    "fig, axs = plt.subplots(figsize=(13*ncols, 11*nrows), ncols=ncols, nrows=nrows)\n",
    "\n",
    "# If we are assessing only one model, add an artificial dimension.\n",
    "if nrows == 1: axs = np.array([axs])\n",
    "# Loop over the models.\n",
    "for model_ax, model_name in zip(axs, model_parameters):\n",
    "    # Get the compositions that the model trained on.\n",
    "    training_comp = model_parameters[model_name]['training_nuclei']\n",
    "    # Get the compositions that the model will be assessing.\n",
    "    assessment_comp = model_parameters[model_name]['assessment_nuclei']\n",
    "    # If we are assessing only one cut, add an artificial dimension.\n",
    "    if ncols == 1: model_ax = np.array([model_ax])\n",
    "    # Loop over the cuts.\n",
    "    for ax, cut_name in zip(model_ax, cut_names):\n",
    "\n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "\n",
    "        # Convert the composition array from storing atomic weights to storing the appropriate output category\n",
    "        categorized_comp = np.vectorize(lambda x: weights_to_categories.get(x))(event_parameters['comp'][events_cut])\n",
    "        \n",
    "        # Create an empty confusion matrix.\n",
    "        confusion_matrix = np.zeros((len(assessment_comp), len(training_comp)), dtype=int)\n",
    "        # Loop over each (true value, prediction) pair.\n",
    "        for true_category, predicted_category in zip(categorized_comp, reconstructions[model_name][reconstructions_cut]):\n",
    "            # Populate the confusion matrix.\n",
    "            confusion_matrix[assessment_comp.index(categories_to_comp.get(true_category)), \n",
    "                             training_comp.index(categories_to_comp.get(predicted_category))] += 1\n",
    "\n",
    "        # Plot the confusion matrix.\n",
    "        im = ax.imshow(confusion_matrix, interpolation='nearest', vmin=0, cmap=plt.cm.Blues)\n",
    "\n",
    "        # Create a colorbar.\n",
    "        cbar = fig.colorbar(im, ax=ax)\n",
    "        cbar.ax.tick_params(**tick_params)\n",
    "        \n",
    "        # Normalize the confusion matrix to get percentages.\n",
    "        normalized_confusion_matrix = confusion_matrix / np.clip(confusion_matrix.sum(axis=1), 1, None)[:, np.newaxis]\n",
    "\n",
    "        # Create a color threshold to switch from light/dark text.\n",
    "        color_threshold = confusion_matrix.max() / 2\n",
    "        # Loop over each square in the confusion matrix.\n",
    "        for row, col in np.ndindex(confusion_matrix.shape):\n",
    "            # Write the text in each box of the confusion matrix.\n",
    "            ax.text(col, row, f'{confusion_matrix[row, col]}\\n{100*normalized_confusion_matrix[row, col]:.2f}%',\n",
    "                    horizontalalignment='center', verticalalignment='center',\n",
    "                    color='white' if confusion_matrix[row, col] > color_threshold else \"black\",\n",
    "                    **text_params)\n",
    "\n",
    "        # Move the true composition label to the top of the plot along with the tick markers\n",
    "        ax.xaxis.set_label_position('top')\n",
    "\n",
    "        # Set the axis tick labels to the string representation of each nuclei\n",
    "        ax.set_xticks(np.arange(len(training_comp)), list(training_comp))\n",
    "        ax.set_yticks(np.arange(len(assessment_comp)), list(assessment_comp))\n",
    "\n",
    "        # Decorate the plot.\n",
    "        ax.set_title(f'{model_name} ({cut_name})', **title_params)\n",
    "        ax.set_xlabel('Predictions', **label_params)\n",
    "        ax.set_ylabel('True Composition', **label_params)\n",
    "        ax.tick_params(**tick_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy as a Function of Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG FOR PLOT\n",
    "cut_names = ['Uncut', 'Quality']\n",
    "energy_range = (5, 8)\n",
    "bin_inc = 0.1\n",
    "marker_size = 640*bin_inc\n",
    "\n",
    "# CONFIG FOR TEXT\n",
    "title_params = {'fontsize':28, 'pad':14}\n",
    "label_params = {'fontsize':20}\n",
    "tick_params = {'axis':'both', 'direction':'in', 'length':12, 'width':2, 'labelsize':16}\n",
    "\n",
    "# VARIABLES FOR PLOT\n",
    "ncols, nrows = len(cut_names), len(model_parameters)\n",
    "energy_min, energy_max = energy_range\n",
    "nbins = int((1/bin_inc)*(energy_max-energy_min))\n",
    "bin_edges = np.linspace(energy_min, energy_max, nbins+1)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "accuracy_min, accuracy_max = 0, 1\n",
    "comp_table = {\n",
    "    'p': {'weight': 1,  'alias': 0, 'name': 'proton', 'color': 'red'},\n",
    "    'h': {'weight': 4,  'alias': 1, 'name': 'helium', 'color': 'orange'},\n",
    "    'o': {'weight': 16, 'alias': 2, 'name': 'oxygen', 'color': 'green'},\n",
    "    'f': {'weight': 56, 'alias': 3, 'name': 'iron',   'color': 'blue'},\n",
    "}\n",
    "\n",
    "# Create the plot/subplots.\n",
    "fig, axs = plt.subplots(figsize=(13*ncols, 10*nrows), ncols=ncols, nrows=nrows)\n",
    "\n",
    "# If we are assessing only one model, add an artificial dimension.\n",
    "if nrows == 1: axs = np.array([axs])\n",
    "# Loop over the models.\n",
    "for model_ax, model_name in zip(axs, model_parameters):\n",
    "    # Save the number of compositions that the model will be assessing.\n",
    "    num_categories = len(model_parameters[model_name]['assessment_nuclei'])\n",
    "    # If we are assessing only one cut, add an artificial dimension.\n",
    "    if ncols == 1: model_ax = np.array([model_ax])\n",
    "    # Loop over the cuts.\n",
    "    for ax, cut_name in zip(model_ax, cut_names):\n",
    "\n",
    "        # Get cuts for the model reconstructions and event parameters.\n",
    "        reconstructions_cut, events_cut = get_cuts(cuts[model_name], event_parameters, cut_name)\n",
    "\n",
    "        # Create lists to store the counts and accuracies for each bin\n",
    "        binned_accuracies, binned_comps = [], []\n",
    "        # Loop over the model-specific assessment composition.\n",
    "        for comp in model_parameters[model_name]['assessment_nuclei']:\n",
    "\n",
    "            # Generate a mask for each nuclei represented as an integer array.\n",
    "            nuclei_mask = (event_parameters['comp'][events_cut] == comp_table[comp]['weight']).astype(int)\n",
    "\n",
    "            # Get all of the nuclei-specific correct predictions.\n",
    "            correct_guesses = (reconstructions[model_name][reconstructions_cut] == comp_table[comp]['alias']) & nuclei_mask\n",
    "\n",
    "            # Bin each nuclei-specific event that was correctly predicted by its energy.\n",
    "            binned_correct_guesses, _ = np.histogram(event_parameters['energy'][events_cut],\n",
    "                                                     bins=bin_edges, weights=correct_guesses)\n",
    "            \n",
    "            # Bin each nuclei-specific event by its energy.\n",
    "            binned_comp, _ = np.histogram(event_parameters['energy'][events_cut],\n",
    "                                          bins=bin_edges, weights=nuclei_mask)\n",
    "            \n",
    "            # Compute the nuclei-specific accuracies for each bin.\n",
    "            # No values in a particular bin means doing a 0/0 division.\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                binned_accuracy = binned_correct_guesses / binned_comp\n",
    "\n",
    "            # Save the nuclei-specific histogram.\n",
    "            binned_comps.append(binned_comp)\n",
    "            # Save the nuclei-specific accuracies.\n",
    "            binned_accuracies.append(binned_accuracy)\n",
    "\n",
    "            # Create scatter plot of the accuracy for each composition.\n",
    "            ax.scatter(bin_centers, binned_accuracy, s=marker_size, color=comp_table[comp]['color'])\n",
    "        \n",
    "        # Create scatter plot of the average accuracies.\n",
    "        ax.scatter(bin_centers, np.mean(binned_accuracies, axis=0), s=marker_size, color='black')\n",
    "\n",
    "        # Create a stacked histogram representing the total number of counts for each bin.\n",
    "        ax.hist(np.repeat(bin_edges[:-1], num_categories).reshape(-1, num_categories),\n",
    "                bins=bin_edges, density=True, weights=np.asarray(binned_comps).transpose(), histtype='barstacked',\n",
    "                color=[comp_table[c]['color'] for c in model_parameters[model_name]['assessment_nuclei']], alpha=0.15)\n",
    "        \n",
    "        # Set the axes limits to be within reasonable ranges.\n",
    "        ax.set_xlim(energy_min-bin_inc, energy_max+bin_inc)\n",
    "        ax.set_ylim(accuracy_min-bin_inc, accuracy_max+bin_inc)\n",
    "        \n",
    "        # Decorate the plot.\n",
    "        ax.set_title(f'{model_name} ({cut_name})', **title_params)\n",
    "        ax.set_xlabel(r'$\\log_{10}(E_{\\mathrm{true}}/\\mathrm{GeV})$', **label_params)\n",
    "        ax.set_ylabel('Accuracy', **label_params)\n",
    "        ax.tick_params(**tick_params)\n",
    "        for axis in ['top','bottom','left','right']:\n",
    "            ax.spines[axis].set_linewidth(2)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IceTop-CNN",
   "language": "python",
   "name": "icetop-cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
