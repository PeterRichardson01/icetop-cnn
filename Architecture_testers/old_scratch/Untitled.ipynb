{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e321c7f7-c647-4e48-8c53-be80be3d1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "​\n",
    "import matplotlib.pyplot\n",
    "import argparse\n",
    "import os\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "​\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "​\n",
    "from Tools.data_tools2 import load_preprocessed, qualityCut, dataPrep, nameModel, name2prep\n",
    "from Tools.extend_data import extend_data\n",
    "from Tools.learningratefinder import LearningRateFinder\n",
    "from Tools.clr_callback import CyclicLR\n",
    "​\n",
    "struct = 'SimplifiedDECO'\n",
    "parser = argparse.ArgumentParser(description = 'Parser')\n",
    "parser.add_argument('key', type=str,help='Key to find model')\n",
    "parser.add_argument('family',type=str,help='Family name of models')\n",
    "parser.add_argument('--lr',type=str,choices=['clr','min','max','mid'],default='mid',help='Choose learning rate type')\n",
    "parser.add_argument('--lrmin',type=float,help='Input min learning rate if not find_lr')\n",
    "parser.add_argument('--lrmax',type=float,help='Input max learning rate if not find_lr')\n",
    "parser.add_argument('--rtheta',default=False,action='store_true',help='Include direction')\n",
    "parser.add_argument('--thetatype',type=str,choices=['laputop','plane','small'],help='Choose which direction values to use.',default='plane')\n",
    "parser.add_argument('--time',default=False,action='store_true',help='Include Time Layers')\n",
    "parser.add_argument('--timetype',type=str,choices=['qmax','false'],help='Method of merging time layers fed to dataPrep')\n",
    "parser.add_argument('--chargetype',type=str,choices=['product','sum','tmin','none'],help='Method of merging charge layers fed to dataPrep')\n",
    "parser.add_argument('--comp',default=False,action='store_true',help='Return composition')\n",
    "parser.add_argument('--remcomp',type=str,nargs='+',help='Choose which composition types to remove: p, h, o, f')\n",
    "parser.add_argument('--normcomp',default=False,action='store_true',help='Normalize composition numbers to between 0 and 1')\n",
    "parser.add_argument('--normalization',default=False,action='store_true')\n",
    "parser.add_argument('--ext_data',type=str,help='Extend data')\n",
    "parser.add_argument('--qcut',type=float,help='Maximum charge for charge cut')\n",
    "parser.add_argument('--zcut',type=float,help='Minimum zenith for zenith cut')\n",
    "parser.add_argument('--bsize',type=int,help='Batch Size',default=512)\n",
    "parser.add_argument('--epochs',type=int,help='Number of epochs',default=500)\n",
    "parser.add_argument('--patience',type=int,help='Early stop patience',default=10)\n",
    "parser.add_argument('--conv_f',type=int,nargs='+',default=[32,64],help='Features per convolutional layer. Input, at most, two values')\n",
    "parser.add_argument('--dense_f',type=int,nargs='+',default=[512,512,512],help='Features per dense layer. Input, at most, three values')\n",
    "parser.add_argument('--conv_do',type=float,nargs='+',default=[.20,.20],help='Dropout for convolutional layers. Input, at most, two values')\n",
    "parser.add_argument('--dense_do',type=float,nargs='+',default=[.20,.20,.20],help='Dropout for dense layers. Input, at most, three values')\n",
    "parser.add_argument('--conv_act',type=float,default=.30,help='Activation for convolutional layers')\n",
    "parser.add_argument('--dense_act',type=float,default=.30,help='Activation for dense layers')\n",
    "args=parser.parse_args()\n",
    "​\n",
    "#Exits program if lr was not specified at all\n",
    "if None in [args.lr,args.lrmin,args.lrmax]:\n",
    "    print('Missing Arguments')\n",
    "    exit(0)\n",
    "​\n",
    "#Sets learning rate according to input\n",
    "if args.lr.lower() =='min':\n",
    "    LR=args.lrmin\n",
    "elif args.lr.lower() == 'max':\n",
    "    LR = args.lrmax\n",
    "else:\n",
    "    LR = 10**((np.log10(args.lrmin)+np.log10(args.lrmax))/2)\n",
    "​\n",
    "#Assigning Feature/Dropout\n",
    "if len(args.conv_f)<2 or len(args.dense_f)<3 or len(args.conv_do)<2 or len(args.dense_do)<3:\n",
    "    print('Missing Features/Dropout Arguments')\n",
    "    exit(0)\n",
    "if len(args.conv_do) == 1:\n",
    "    args.conv_do = [args.conv_do[0] for i in args.conv_f]\n",
    "if len(args.dense_do) == 1:\n",
    "    args.dense_do = [args.dense_do[0] for i in args.dense_f]\n",
    "​\n",
    "#Storing Model\n",
    "filepath = '/home/pascali_b/MachineLearning/Models'\n",
    "directoryname = '{}/{}/{}/{}'.format(filepath,struct,args.family,args.key)\n",
    "if not os.path.exists(directoryname):\n",
    "    os.makedirs(directoryname)\n",
    "    print('Checkpoint Directory '+directoryname+ ' Created')\n",
    "#Create Configuration file\n",
    "cfg_file = 'config.txt'\n",
    "cfg_file = '{}/{}'.format(directoryname, cfg_file)\n",
    "​\n",
    "complist=['p','h','o','f']\n",
    "if args.remcomp !=None:    \n",
    "    for i in args.remcomp:\n",
    "        if i.lower() in complist:\n",
    "            complist.remove(i)\n",
    "print(complist)\n",
    "​\n",
    "#Data Preprocessing\n",
    "simPrefix = '/home/pascali_b/MachineLearning/Data/Preprocessed4/preprocessed'\n",
    "x, y = load_preprocessed(simPrefix, 'train',comp=complist)\n",
    "​\n",
    "if args.rtheta==True:\n",
    "    if args.thetatype=='plane':\n",
    "        rtheta, _ = y['plane_dir'].transpose()\n",
    "    elif args.thetatype=='laputop':\n",
    "        rtheta, _ = y['laputop_dir'].transpose()\n",
    "    elif args.thetatype=='small':\n",
    "        rtheta, _ = y['small_dir'].transpose()\n",
    "​\n",
    "# Optional cuts\n",
    "cut = np.ones(x.shape[0], dtype=bool)\n",
    "​\n",
    "#Quality Cut\n",
    "if args.qcut != None or args.zcut != None:\n",
    "    cut *= qualityCut(x, rtheta, qmax=args.qmax, zmax=args.zmax)\n",
    "​\n",
    "#Theta Cut\n",
    "if args.rtheta==True:\n",
    "    cut *= (rtheta!=None) * (rtheta==rtheta)\n",
    "​\n",
    "#Cosine\n",
    "if args.rtheta==True and args.cos==True:\n",
    "    rtheta=np.cos(rtheta)\n",
    "​\n",
    "# Data preparation\n",
    "x = x[cut]\n",
    "if args.comp ==False:\n",
    "    y = y['energy'][cut]\n",
    "    y = y.reshape(-1,1)\n",
    "elif args.comp == True:\n",
    "    if args.normcomp==True:\n",
    "        y['comp']=((y['comp']-1)/55)\n",
    "    y1 = y['energy'][cut]\n",
    "    y2 = y['comp'][cut]\n",
    "    y = np.column_stack((y1,y2))\n",
    "    print('Comp Included')\n",
    "if args.rtheta==True:\n",
    "    rtheta = rtheta[cut].astype(float)\n",
    "    rtheta = rtheta.reshape(-1,1)\n",
    "    rtheta = np.pi-rtheta\n",
    "if args.rtheta==False:\n",
    "    args.thetatype=False\n",
    "    rtheta=np.ones(2)\n",
    "#Time\n",
    "if args.time==False:\n",
    "    args.timetype=False\n",
    "#Prep Data\n",
    "prep = {'q':args.chargetype,'t':args.timetype,'normed':args.normalization,'reco':args.thetatype}\n",
    "if args.rtheta==True:\n",
    "    x,rtheta = dataPrep(x,rtheta,q=args.chargetype,t=args.timetype, normed=args.normalization)\n",
    "if args.rtheta==False:\n",
    "    x,nothing= dataPrep(x,rtheta,q=args.chargetype,t=args.timetype, normed=args.normalization)\n",
    "​\n",
    "# Extrapolate Data\n",
    "#if args.ext_data != None:\n",
    "    #x, y = extend_data(x, y, args.ext_data)\n",
    "#x,y = extend_data(x,y,'2x')\n",
    "​\n",
    "#85/15 split for training/testing and remove nans\n",
    "nevents = y.shape[0]\n",
    "trainCut = (np.random.uniform(size=nevents) < 0.85)\n",
    "testCut = np.logical_not(trainCut)\n",
    "​\n",
    "#Create Model\n",
    "ip1 = keras.Input(shape=x[0].shape)\n",
    "for i,feat in enumerate(args.conv_f):\n",
    "    if i ==0:\n",
    "        l = layers.Conv2D(args.conv_f[i],kernel_size=3,padding='same',use_bias=False)(ip1)\n",
    "    else:\n",
    "        l = layers.Conv2D(args.conv_f[i],kernel_size=3,padding='same',use_bias=False)(l)\n",
    "    l = layers.BatchNormalization()(l)\n",
    "    l = layers.LeakyReLU(alpha=args.conv_act)(l)\n",
    "    l = layers.Conv2D(args.conv_f[i],kernel_size=3,padding='same',use_bias=False)(l)\n",
    "    l = layers.BatchNormalization()(l)\n",
    "    l = layers.LeakyReLU(alpha=args.conv_act)(l)\n",
    "    l = layers.Dropout(args.conv_do[i])(l)\n",
    "if args.rtheta==True:\n",
    "    l = layers.Flatten()(l)\n",
    "    ip2 = keras.Input(shape=rtheta[0].shape)\n",
    "    merge = layers.Concatenate()([l,ip2])\n",
    "else:\n",
    "    merge=layers.Flatten()(l)\n",
    "for i,feat in enumerate(args.dense_f):\n",
    "    if i ==0:\n",
    "        l = layers.Dense(args.dense_f[i],use_bias=False)(merge)\n",
    "    else:\n",
    "        l = layers.Dense(args.dense_f[i],use_bias=False)(l)\n",
    "    l = layers.BatchNormalization()(l)\n",
    "    l = layers.LeakyReLU(alpha=args.dense_act)(l)\n",
    "    l = layers.Dropout(args.dense_do[i])(l)\n",
    "if args.comp==False:\n",
    "    output = layers.Dense(1)(l)\n",
    "if args.comp == True:\n",
    "    output = layers.Dense(2)(l)\n",
    "if args.rtheta==True:\n",
    "    model = keras.Model(inputs = [ip1,ip2], outputs = output,name=nameModel(prep,prefix='brandon'))\n",
    "elif args.rtheta==False:\n",
    "    model = keras.Model(inputs = ip1, outputs = output,name=nameModel(prep,prefix='brandon'))\n",
    "​\n",
    "#Create Configuration file\n",
    "cfg_file = 'config.txt'\n",
    "cfg_file = '{}/{}'.format(directoryname, cfg_file)\n",
    "with open(cfg_file, 'w') as config:\n",
    "    config.write('Configuration file for {} in {} \\n'.format(args.key, args.family))\n",
    "    config.write('Learning Rate is '+str(LR)+'\\n')\n",
    "    config.write('Learning Rate Method is '+args.lr+'\\n')\n",
    "    config.write('Learning Rate Min: ' + str(args.lrmin) + '\\n')\n",
    "    config.write('Learning Rate Max: ' + str(args.lrmax) + '\\n')\n",
    "    config.write('Normalization? ' +str(args.normalization)+'\\n')\n",
    "    config.write('Quality Cut Charge is '+str(args.qcut)+'\\n')\n",
    "    config.write('Zenith Cut is '+str(args.zcut)+'\\n')\n",
    "    config.write('Direction? '+str(args.rtheta)+'\\n')\n",
    "    config.write('Direction Type is: '+str(args.thetatype)+'\\n')\n",
    "    config.write('Time? '+str(args.time)+'\\n')\n",
    "    config.write('Time type is '+str(args.timetype)+'\\n')\n",
    "    config.write('ChargeType is '+str(args.chargetype)+'\\n')\n",
    "    config.write('Composition? '+str(args.comp)+'\\n')\n",
    "    for i in complist:\n",
    "        config.write(str(i)+' included'+'\\n')                                               \n",
    "    config.write('Extrapolated Data? '+str(args.ext_data)+'\\n')                             \n",
    "    config.write('Number of Epochs is '+str(args.epochs)+'\\n')\n",
    "    config.write('Early Stop Patience is '+str(args.patience)+'\\n')\n",
    "    for i, n in enumerate(args.conv_f):\n",
    "        config.write('Double Convolution Layer {} Features: {}\\n'.format(i, n))\n",
    "    for i, n in enumerate(args.dense_f):\n",
    "        config.write('Dense Layer {} Features: {}\\n'.format(i, n))\n",
    "    for i, n in enumerate(args.conv_do):\n",
    "        config.write('Double Convolution Layer {} Dropout: {}\\n'.format(i, n))\n",
    "    for i, n in enumerate(args.dense_do):\n",
    "        config.write('Dense Layer {} Dropout: {}\\n'.format(i, n)) \n",
    "    config.write('Convolution Activation: {}\\n'.format(args.conv_act))\n",
    "    config.write('Dense Activation: {}\\n'.format(args.dense_act))\n",
    "    model.summary(print_fn=lambda x: config.write(x + '\\n'))\n",
    "    config.close()\n",
    "​\n",
    "#Callbacks/save model weights at epochs\n",
    "csv_logger = CSVLogger('{}/training_log.csv'.format(directoryname))\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience=args.patience,restore_best_weights=True)\n",
    "​\n",
    "opt = Adam(lr=LR)\n",
    "model.compile(optimizer=opt, loss='mean_squared_error', metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "​\n",
    "print('[INFO] training network...')\n",
    "callbacks = [early_stop, csv_logger]\n",
    "if args.rtheta==True:\n",
    "    H = model.fit(x =[x[trainCut],rtheta[trainCut]],y=y[trainCut],batch_size=args.bsize,validation_data=([x[testCut],rtheta[testCut]], y[testCut]),epochs=args.epochs,callbacks=callbacks)\n",
    "else:\n",
    "    H = model.fit(x =x[trainCut],y=y[trainCut],batch_size=args.bsize,validation_data=(x[testCut], y[testCut]),epochs=args.epochs,callbacks=callbacks)\n",
    "# Save model to file\n",
    "name1=nameModel(prep,prefix='brandon')\n",
    "model.save('{}/{}.h5'.format(directoryname,name1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
