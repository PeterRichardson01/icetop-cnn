{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE \"HELLO WORLD\" OF MACHINE LEARNING - MNIST CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Import Required Module(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow is the machine learning framework. It is built off of the Keras API.\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Load the Images and Labels\n",
    "For now, we only need the training dataset. \\\n",
    "It is crucial that the training dataset and the testing dataset are never combined in any capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The images are what is fed into the model as input. All images are just arrays of pixel values (e.g., RGBA).\n",
    "# The labels correspond to the digit 0-9 represented in each image. These are used to both train the model and compare its answers.\n",
    "(training_images, training_labels), _ = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the images from using unsigned 8-bit integers to 32-bit floats, then normalize.\n",
    "# This step alone increases the final accuracy of the model by approximately 5% of the dataset.\n",
    "training_images = training_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are building a fairly straightforward model, we can use the Sequential API provided by Keras.\n",
    "# For more complex models, use the Functional API.\n",
    "model = tf.keras.models.Sequential([\n",
    "    # The input layer is of the same dimension as the images. MNIST uses 28x28 pixel images.\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    # The hidden layer(s), or \"meat\" of the neural network. In general, more hidden layers means a more capable or complex network.\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    # For classification, the number of nodes in the output layer corresponds to the number of categories, or in this case, digits.\n",
    "    tf.keras.layers.Dense(10)                      \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the selected optimizer, loss function, and metrics to analyze during training.\n",
    "model.compile(\n",
    "    # Determines how model weights are updated in response to the loss function.\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),                            \n",
    "    # Determines how harshly to penalize incorrect model predictions.\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    # Metrics do not update model weights. They are only used for callbacks and sanity checks.\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],               \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify input and target values to the model, along with parameters such as batch size.\n",
    "history = model.fit(\n",
    "    training_images,        # The images for the model to train on.\n",
    "    training_labels,        # The correct labels for the training images.\n",
    "    batch_size=2**7,        # Number of samples before the model updates its parameters.\n",
    "    epochs=6,               # How many times the model runs over a dataset.\n",
    "    validation_split=0.20,  # The percentage of training data to be used as validation data.\n",
    "    shuffle=True            # Boolean indicating whether the input should be shuffled between each epoch.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Import Required Module(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy is an advanced, highly optimized mathematical library\n",
    "import numpy as np\n",
    "# Matplotlib is a plotting module that allows us to easily visualize data.\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Load the Images and Labels\n",
    "Now, we need the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In testing, only the images are provided to the model, not the labels.\n",
    "# The labels now are for the user to compare with the models' predictions.\n",
    "_, (testing_images, testing_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Data Preprocessing (notice a pattern?)\n",
    "It is crucial that the testing data is processed exactly the same as the training data. \\\n",
    "Otherwise, the results of the model will be misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once again, cast the images from using unsigned 8-bit integers to 32-bit floats, then normalize.\n",
    "testing_images = testing_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Predict Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As mentioned previously, only the testing images are provided to the model for predictions.\n",
    "# Providing the labels would be cheating! Providing training data would also be cheating, since the model has already seen those images.\n",
    "predictions = model.predict(testing_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Process Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The format of the predictions output matches the format of the output of the model.\n",
    "# Since the model has 10 output nodes, the result of each prediction is a list of 10 elements.\n",
    "# The way to intepret this prediction list is that each element is a likelihood value corresponding to each possible output category, or digit.\n",
    "# The index of each likelihood value in the list corresponds to the digit whose likelihood it is representing.\n",
    "# High-magnitude likelihood values correspond to strong model confidence in that particular category.\n",
    "# Positive likelihood values indicate the model \"agrees\" with a particular digit, and vice-versa.\n",
    "# The digit that the model predicts for each image is the one with the most positive likelihood value.\n",
    "discretized_predictions = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Analyze Results\n",
    "We have arrived to the final step in the machine learning process - assessing model performance on unseen data. \\\n",
    "This is mostly a sandbox at this point where we try to look for trends in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predictions = len(discretized_predictions)\n",
    "num_successful_predictions = sum(discretized_predictions == testing_labels)\n",
    "\n",
    "print(f'Total Accuracy:  {100 * num_successful_predictions / num_predictions:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Category Specific Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for digit in range(10):\n",
    "    digit_specific_image_ids = testing_labels == digit\n",
    "    num_digit_specific_predictions = sum(digit_specific_image_ids)\n",
    "    num_successful_digit_specific_predictions = sum(discretized_predictions[digit_specific_image_ids] == testing_labels[digit_specific_image_ids])\n",
    "\n",
    "    print(f'Accuracy for {digit}:  {100 * num_successful_digit_specific_predictions / num_digit_specific_predictions:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction_image_ids = np.nonzero(discretized_predictions == testing_labels)[0]\n",
    "\n",
    "random_correct_image_id = np.random.choice(correct_prediction_image_ids)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title(f'Prediction: {discretized_predictions[random_correct_image_id]}        True Value: {testing_labels[random_correct_image_id]}')\n",
    "plt.imshow(testing_images[random_correct_image_id].reshape(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Problematic Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_prediction_image_ids = np.nonzero(discretized_predictions != testing_labels)[0]\n",
    "\n",
    "print(incorrect_prediction_image_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Much Better Can WE Do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_incorrect_image_id = np.random.choice(incorrect_prediction_image_ids)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title(f'Prediction: {discretized_predictions[random_incorrect_image_id]}        True Value: {testing_labels[random_incorrect_image_id]}')\n",
    "plt.imshow(testing_images[random_incorrect_image_id].reshape(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Website:\n",
    "# playground.tensorflow.org"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
